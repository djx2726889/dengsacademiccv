<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiaxin Deng (ÈÇìÂòâÈë´)</title>
  
  <meta name="author" content="Jiaxin Deng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jiaxin Deng (ÈÇìÂòâÈë´)</name>
              </p>
              <p>I am a master student at <a href="http://www.ia.cas.cn/">Institute of Automation</a> in Beijing, where I work on video multi-modal understanding and time series mining.
              </p>
              <p>
		I did my undergraduate degree at <a href="https://www.uestc.edu.cn/">UESTC</a> and I am doing my Master's Degree at <a href="http://www.ia.cas.cn/">Institute of Automation</a>, where I am advised by <a href="https://people.ucas.ac.cn/~gfmeng">Gaofeng Meng</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:2532088315@qq.com">Email</a> &nbsp/&nbsp
                <a href="data/DengJiaXin_UESTC_CH.pdf">CV</a> &nbsp/&nbsp
                <a href="data/dengjiaxin_bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://twitter.com/orpheusann">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/djx2726889/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/web_icon.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/web_icon.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
		I'm interested in multi-modal video understanding, continual learning, and time series mining. Much of my research is about mutli-modal learning (visual, text, speech, etc) from videos. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<tr onmouseout="cikm_stop()" onmouseover="cikm_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cikm_image'><img src='images/kddfinal.jpg' width="180" ></div>
                <img src='images/kddfinal.jpg' width="180">
              </div>
              <script type="text/javascript">
                function cikm_start() {
                  document.getElementById('cikm_image').style.opacity = "1";
                }

                function cikm_stop() {
                  document.getElementById('cikm_image').style.opacity = "0";
                }
                cikm_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.dropbox.com/s/x9elxihl071mibk/KDD2024.pdf?dl=0">
                <papertitle>MMBee: Live Streaming Gift-Sending Recommendations via Multi-Modal Fusion and Behaviour Expansion</papertitle>
              </a>
              <br>
              <strong>Jiaxin Deng</strong>,
	      <a href="https://scholar.google.com/citations?user=YdEpCI4AAAAJ&hl=en">Shiyao Wang</a>,
              Yuchen Wang,
	      Jiansong Qi,
              Liqin Zhao,
              <a href="https://scholar.google.com/citations?user=n_E0Bg4AAAAJ&hl=en">Guorui Zhou</a>,
              <a href="https://scholar.google.com/citations?user=5hti_r0AAAAJ&hl=zh-CN">Gaofeng Meng*</a>
              <br>
              <em>KDD</em>, 2024 &nbsp <font color="red"><strong>(Just Accepted!)</strong></font>
              <br>
	      <a >paper</a> /
	      <a >slides</a> /
              <a >video</a> 
              <p></p>
              <p>Live streaming services are becoming increasingly popular due to real-time interactions and entertainment. Viewers can chat and send comments or virtual gifts to express their preferences for the streamers. Accurately modeling the gifting interaction not only enhances users' experience but also increases streamers' revenue. Previous studies on live streaming gifting prediction treat this task as a conventional recommendation problem, and model users' preferences using categorical data and observed historical behaviors. However, it is challenging to precisely describe the real-time content changes in live streaming using limited categorical information. Moreover, due to the sparsity of gifting behaviors, capturing the preferences and intentions of users is quite difficult. In this work, we propose MMBee based on real-time Multi-Modal Fusion and Behaviour Expansion to address these issues. Specifically, we first present a Multi-modal Fusion Module with Learnable Query (MFQ) to perceive the dynamic content of streaming segments and process complex multi-modal interactions, including images, text comments and speech. To alleviate the sparsity issue of gifting behaviors, we present a novel Graph-guided Interest Expansion (GIE) approach that learns both user and streamer representations on large-scale gifting graphs with multi-modal attributes. It consists of two main parts: graph node representations pre-training and metapath-based behavior expansion, all of which help model jump out of the specific historical gifting behaviors for exploration and largely enrich the behavior representations.Comprehensive experiment results show that MMBee achieves significant performance improvements on both public datasets and Kuaishou real-world streaming datasets and the effectiveness has been further validated through online A/B experiments. MMBee has been deployed and is serving hundreds of millions of users at Kuaishou.</p>
            </td>
          </tr>

	<tr onmouseout="cikm_stop()" onmouseover="cikm_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cikm_image'><img src='images/icme24.jpg' width="180" ></div>
                <img src='images/icme24.jpg' width="180">
              </div>
              <script type="text/javascript">
                function cikm_start() {
                  document.getElementById('cikm_image').style.opacity = "1";
                }

                function cikm_stop() {
                  document.getElementById('cikm_image').style.opacity = "0";
                }
                cikm_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.dropbox.com/scl/fi/iohm029w6cqxgrog1bpdu/icmefinal.pdf?rlkey=vsr3kz6aun1bybr4vsxu9zv77&dl=0">
                <papertitle>A Multimodal Transformer for Live Streaming Highlight Prediction</papertitle>
              </a>
              <br>
              <strong>Jiaxin Deng</strong>,
	      <a href="https://scholar.google.com/citations?user=YdEpCI4AAAAJ&hl=en">Shiyao Wang</a>,
              Dong Shen,
              Liqin Zhao,
              Fan Yang,
              <a href="https://scholar.google.com/citations?user=n_E0Bg4AAAAJ&hl=en">Guorui Zhou</a>,
              <a href="https://scholar.google.com/citations?user=5hti_r0AAAAJ&hl=zh-CN">Gaofeng Meng*</a>
              <br>
              <em>ICME</em>, 2024 &nbsp <font color="red"><strong>(Poster)</strong></font>
              <br>
	      <a >paper</a> /
	      <a >slides</a> /
              <a >video</a> 
              <p></p>
              <p>Recently, live streaming platforms have gained immense popularity. Traditional video highlight detection mainly focuses on visual features and utilizes both past and future content for prediction. However, live streaming requires models to infer without future frames and process complex multimodal interactions, including images, audio and text comments. To address these issues, we propose a multimodal transformer that incorporates historical look-back windows. We introduce a novel Modality Temporal Alignment Module to handle the temporal shift of cross-modal signals. Additionally, using existing datasets with limited manual annotations is insufficient for live streaming whose topics are constantly updated and changed. Therefore, we propose a novel Border-aware Pairwise Loss to learn from a large-scale dataset and utilize user implicit feedback as a weak supervision signal. Extensive experiments show our model outperforms various strong baselines on both real-world scenarios and public datasets.</p>
            </td>
          </tr>
				

          <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/icmr_all.png' width="160"></div>
                <img src='images/icmr_all.png' width="160">
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/10.1145/3591106.3592258">
                <papertitle>A Unified Model for Video Understanding and Knowledge Embedding with Heterogeneous Knowledge Graph Dataset</papertitle>
              </a>
              <br>
              <strong>Jiaxin Deng</strong>,
	      Dong Shen,
              <a href="https://scholar.google.com/citations?user=2NehWz0AAAAJ&hl=zh-CN">Haojie Pan</a>,
              Xiangyu Wu,
              Ximan Liu,
              <a href="https://scholar.google.com/citations?user=5hti_r0AAAAJ&hl=zh-CN">Gaofeng Meng*</a>,
              Fan Yang,
              Tingting Gao,
              <a href="https://scholar.google.com/citations?user=vhCLl50AAAAJ&hl=en">Ruiji Fu</a>,
              <a href="https://scholar.google.com/citations?user=4XVJrRAAAAAJ&hl=en">Zhongyuan Wang</a>
              <br>
              <em>ICMR</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Candidate)</strong></font>
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3591106.3592258">paper</a> /
	      <a href="https://www.dropbox.com/s/wpm4b387zgodj1l/presentation.pdf?dl=0">slides</a> /
              <a href="https://www.dropbox.com/s/qsou4k286fhjnkw/icmr_jiaxindeng.mp4?dl=0">video</a> 
              <p></p>
              <p>We propose a heterogeneous dataset that contains the multi-modal video entity and fruitful common sense relations. This dataset also provides multiple novel video inference tasks like the Video-Relation-Tag (VRT) and Video-Relation-Video (VRV) tasks. Furthermore, based on this dataset, we propose an end-to-end model that jointly optimizes the video understanding objective with knowledge graph embedding, which can not only better inject factual knowledge into video understanding but also generate effective multi-modal entity embedding for KG.</p>
            </td>
          </tr>



        </tbody></table>


					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
		Please <strong>refrain</strong> from scraping the HTML of this page, as it contains analytics tags that are undesirable for your own website.
              </p>
            </td>
          </tr>
        </tbody></table>


	</tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=7U-NbkNueSQCnWoXzOHtuI1VaiAdSH-jN7XGn6nE3Qg'></script>
            </td>
          </tr>
        </tbody></table>

	
      </td>
    </tr>
  </table>
</body>

</html>
